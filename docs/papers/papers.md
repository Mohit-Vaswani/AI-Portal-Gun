---
hide:
  - navigation
---

# Research Papers

Below, you'll find the latest, highly-cited papers, organized by their release date. We keep this list updated on a weekly basis for the most current research.

<!-- <img src="/assets/images/r&dMeme.png" alt="R&D meme" /> -->

- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (Oct 2023)
- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs//2310.06117) (Oct 2023)
- [Introducing The Foundation Model Transparency Index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
- [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)
- [Habitat 3.0: A Co-Habitat for Humans, Avatars and RobotsHabitat 3.0: A Co-Habitat for Humans, Avatars and Robots](https://aihabitat.org/habitat3/)
- [Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections](https://selfrag.github.io/) (Oct 2023)
- [Improved Baselines with Visual Instruction Tuning](https://browse.arxiv.org/pdf/2310.03744.pdf) (Oct 2023)
- [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/scaling-up-learning-across-many-different-robot-types/Open_X_Embodiment__Robotic_Learning_Datasets_and_RT_X_Models.pdf) (Oct 2023)
- [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://arxiv.org/pdf/2310.02304.pdf) (Oct 2023)
- [RealFill: Reference-Driven Generation for Authentic Image Completion](https://arxiv.org/abs/2309.16668) (Sept 2023)

## LLMs

## Generative AI

### Image Generation

### Audio Generation

### Video Generation

### Code Generation

### Multimodel

### Prompt Engineering

- [Parameter-Efficient Transfer Learning for NLP](https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf) [(project)](https://github.com/google-research/adapter-bert)

- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) [(project)](https://github.com/google-research/text-to-text-transfer-transformer)

- [Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066) [(pdf)](https://github.com/facebookresearch/LAMA)


## Robotics

- [Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots](https://ai.meta.com/static-resource/habitat3) (2023) [(website)](https://aihabitat.org/habitat3/) 

- [PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703v1) (2019) [(code)](https://github.com/AtsushiSakai/PythonRobotics)

### Key Papers

- [A Practical and Effective Layout for a Safe Human-Robot Collaborative Assembly Task](https://doi.org/10.3390/app11041763) (2021)
- [Control of DC Motors to Guide Unmanned Underwater Vehicles](https://doi.org/10.3390/app11052144) (2021)
- [Belt Conveyors Rollers Diagnostics Based on Acoustic Signal Collected Using Autonomous Legged Inspection Robot](https://doi.org/10.3390/app11052299) (2021)
- [Deep Reinforcement Learning for the Control of Robotic Manipulation: A Focussed Mini-Review](https://www.mdpi.com/2218-6581/3/4/57) (2019)
- [Steps Towards the Object Semantic Hierarchy](https://repositories.lib.utexas.edu/handle/2152/ETD-UT-2011-05-2927) (2011)
- [Topological Mapping and Navigation in Real-World Environments](https://deepblue.lib.umich.edu/handle/2027.42/146710) (2018)
- [A tale of two architectures: A dual-citizenship integration of natural language and the cognitive map](https://link.springer.com/article/10.1007/s10458-017-9364-8) (2017)
- [Creating and Utilizing Symbolic Representations of Spatial Knowledge using Mobile Robots](https://repositories.lib.utexas.edu/handle/2152/ETD-UT-2008-05-305) (2008)
- [A robust layered control system for a mobile robot](https://ieeexplore.ieee.org/document/547205) (1996)
- [Probabilistic robotics](https://www.cs.cmu.edu/~thrun/probrob-12.pdf) (2005)

## Healthcare & Biology

- [Alphafold](https://www.deepmind.com/research/highlighted-research/alphafold) (2020)

## Approaches

## Reinforcement Learning
- [Never Give Up: Learning Directed Exploration Strategies](https://arxiv.org/abs/2002.06038) (2020)

####  Key Papers 

- [Q-learning](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf) (1992): Introduces the Q-learning algorithm, one of the fundamental algorithms in RL.
- [Policy invariance under reward transformations: Theory and application to reward shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf) (1999): Discusses the invariance of policies under reward transformations and the concept of reward shaping.
- [Learning to Predict by the Methods of Temporal Differences](https://www.researchgate.net/publication/225264698_Learning_to_Predict_by_the_Method_of_Temporal_Differences) (1988): Introduced the temporal difference (TD) learning algorithm, which is a model-free method for learning value functions in RL.
- [Actor-Critic Algorithms](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) (2003): Introduced the actor-critic architecture, which is a model-based method for learning policies in RL.



### Deep RL

####  Key Papers 

- [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (2013): Presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.
- [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/abs/1507.06527) (2015): Proposes a deep recurrent Q-learning algorithm for partially observable Markov decision processes.
- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) (2015): Introduces a dueling network architecture for deep reinforcement learning that separates the estimation of state values and state-dependent action advantages.
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (2015): Proposes a double Q-learning algorithm for deep reinforcement learning that reduces overestimation of action values.
- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) (2015): Introduces a prioritized experience replay mechanism for deep reinforcement learning that improves sample efficiency and learning speed.
- [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298) (2017): Combines several improvements to deep reinforcement learning, including dueling networks, double Q-learning, and prioritized experience replay, to achieve state-of-the-art performance on Atari games.
- [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783) (2016): Proposes asynchronous methods for deep reinforcement learning that improve sample efficiency and learning speed.
- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) (2015): Introduces a trust region optimization method for policy optimization in reinforcement learning that improves stability and sample efficiency.
- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) (2015): Proposes a generalized advantage estimation method for continuous control tasks in reinforcement learning that improves sample efficiency and learning speed.
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) (2017): Introduces a family of proximal policy optimization algorithms for reinforcement learning that improve sample efficiency and stability.
- [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286) (2017): Demonstrates the emergence of diverse locomotion behaviors in simulated environments using deep reinforcement learning.
- [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation](https://arxiv.org/abs/1708.05144) (2017): Proposes a scalable trust-region method for deep reinforcement learning that uses Kronecker-factored approximation to improve sample efficiency and learning speed.
- [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224) (2016): Introduces a sample-efficient actor-critic algorithm with experience replay for reinforcement learning that improves sample efficiency and learning speed.
- [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290) (2018): Proposes a soft actor-critic algorithm for deep reinforcement learning that maximizes entropy and improves exploration.
- [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf) (2014): Introduces a deterministic policy gradient algorithm for reinforcement learning that improves sample efficiency and stability.
- [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) (2015): Demonstrates the effectiveness of deep reinforcement learning for continuous control tasks.
- [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) (2018): Addresses the problem of function approximation error in actor-critic methods for reinforcement learning.
- [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) (2017): Presents a distributional perspective on reinforcement learning that improves sample efficiency and learning speed.
- [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044) (2017): Proposes a distributional reinforcement learning algorithm that uses quantile regression to estimate value distributions.
- [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923) (2018): Introduces implicit quantile networks for distributional reinforcement learning that improve sample efficiency and learning speed.
- [Dopamine: A Research Framework for Deep Reinforcement Learning](https://openreview.net/forum?id=ByG_3s09KX) (2018) [(code)](https://github.com/google/dopamine): Provides a research framework for deep reinforcement learning that includes a suite of environments and baselines.
- [Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic](https://arxiv.org/abs/1611.02247) (2016): Proposes a sample-efficient policy gradient algorithm with an off-policy critic for reinforcement learning that improves sample efficiency and learning speed.
- [Action-depedent Control Variates for Policy Optimization via Stein’s Identity,](https://arxiv.org/abs/1710.11198) (2017): Proposes a control variate method for policy optimization in reinforcement learning that improves sample efficiency and stability.
- [The Mirage of Action-Dependent Baselines in Reinforcement Learning](https://arxiv.org/abs/1802.10031) (2018): Critiques the use of action-dependent baselines in reinforcement learning and proposes alternative methods.
- [Bridging the Gap Between Value and Policy Based Reinforcement Learning](https://arxiv.org/abs/1702.08892) (2017): Proposes a method for bridging the gap between value-based and policy-based reinforcement learning.
- [Trust-PCL: An Off-Policy Trust Region Method for Continuous Control](https://arxiv.org/abs/1707.01891) (2017): Introduces an off-policy trust region method for continuous control in reinforcement learning that improves sample efficiency and stability.
- [Combining Policy Gradient and Q-learning](https://arxiv.org/abs/1611.01626) (2016): Combines policy gradient and Q-learning methods for reinforcement learning to improve sample efficiency and stability.
- [The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning](https://arxiv.org/abs/1704.04651) (2017): Introduces a fast and sample-efficient actor-critic algorithm for reinforcement learning that improves sample efficiency and learning speed.
- [Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning](http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning) (2017): Proposes an interpolated policy gradient algorithm for deep reinforcement learning that combines on-policy and off-policy gradient estimation.
- [Equivalence Between Policy Gradients and Soft Q-Learning](https://arxiv.org/abs/1704.06440) (2017): Shows the equivalence between policy gradients and soft Q-learning in reinforcement learning.
- [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864) (2017): Explores the use of evolution strategies, a class of black box optimization algorithms, as an alternative to popular reinforcement learning techniques.
- [VIME: Variational Information Maximizing Exploration](https://arxiv.org/abs/1605.09674) (2016): Proposes a variational information maximizing exploration method for reinforcement learning that improves exploration efficiency.
- [Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868) (2016): Unifies count-based exploration and intrinsic motivation methods for reinforcement learning to improve exploration efficiency.
- [Count-Based Exploration with Neural Density Models](https://arxiv.org/abs/1703.01310) (2017): Proposes a count-based exploration method for reinforcement learning that uses neural density models to improve exploration efficiency.
- [#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning](https://arxiv.org/abs/1611.04717) (2016): Studies the effectiveness of count-based exploration methods for deep reinforcement learning.
- [EX2: Exploration with Exemplar Models for Deep Reinforcement Learning](https://arxiv.org/abs/1703.01260) (2017): Proposes an exploration method for deep reinforcement learning that uses exemplar models to improve exploration efficiency.
- [Curiosity-driven Exploration by Self-supervised Prediction](https://arxiv.org/abs/1705.05363) (2017): Proposes a curiosity-driven exploration method for reinforcement learning that uses self-supervised prediction to improve exploration efficiency.
- [Large-Scale Study of Curiosity-Driven Learning](https://arxiv.org/abs/1808.04355) (2018): Conducts a large-scale study of curiosity-driven learning in reinforcement learning.
- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894) (2018): Proposes an exploration method for reinforcement learning that uses random network distillation to improve exploration efficiency.
- [Variational Intrinsic Control](https://arxiv.org/abs/1611.07507) (2016): Proposes a variational intrinsic control method for reinforcement learning that improves exploration efficiency.
- [Diversity is All You Need: Learning Skills without a Reward Function](https://arxiv.org/abs/1802.06070) (2018): Proposes a method for learning skills without a reward function in reinforcement learning that improves sample efficiency.
- [Variational Option Discovery Algorithms](https://arxiv.org/abs/1807.10299) (2018): Proposes a variational option discovery algorithm for reinforcement learning that improves sample efficiency.
- [Progressive Neural Networks](https://arxiv.org/abs/1606.04671) (2016): Proposes a progressive neural network architecture for reinforcement learning that improves sample efficiency.
- [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf) (2015): Proposes a universal value function approximator for reinforcement learning that improves sample efficiency.
- [The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously](https://arxiv.org/abs/1707.03300) (2017): Proposes a method for learning to solve multiple continuous control tasks simultaneously in reinforcement learning.
- [PathNet: Evolution Channels Gradient Descent in Super Neural Networks](https://arxiv.org/abs/1701.08734) (2017): Proposes a method for combining evolution and gradient descent in neural network training for reinforcement learning.
- [Mutual Alignment Transfer Learning](https://arxiv.org/abs/1707.07907) (2017): Proposes a mutual alignment transfer learning method for reinforcement learning that improves sample efficiency.
- [Learning an Embedding Space for Transferable Robot Skills](https://openreview.net/forum?id=rk07ZXZRb&noteId=rk07ZXZRb) (2018): Proposes a method for learning an embedding space for transferable robot skills in reinforcement learning.
- [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (2017): Proposes a hindsight experience replay method for reinforcement learning that improves sample efficiency.
- [Strategic Attentive Writer for Learning Macro-Actions](https://arxiv.org/abs/1606.04695) (2016): Proposes a strategic attentive writer method for learning macro-actions in reinforcement learning that improves sample efficiency.
- [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161) (2017): Proposes a feudal network architecture for hierarchical reinforcement learning that improves sample efficiency.
- [Data-Efficient Hierarchical Reinforcement Learning](https://arxiv.org/abs/1805.08296)
  (2018): Proposes a data-efficient hierarchical reinforcement learning method that improves sample efficiency.
- [Model-Free Episodic Control](https://arxiv.org/abs/1606.04460) (2016): Proposes a model-free episodic control method for reinforcement learning that improves sample efficiency.
- [Neural Episodic Control](https://arxiv.org/abs/1703.01988) (2017): Proposes a neural episodic control method for reinforcement learning that improves sample efficiency.
- [Neural Map: Structured Memory for Deep Reinforcement Learning](https://arxiv.org/abs/1702.08360) (2017): Proposes a neural map architecture for reinforcement learning that uses structured memory to improve sample efficiency.
- [Unsupervised Predictive Memory in a Goal-Directed Agent](https://arxiv.org/abs/1803.10760) (2018): Proposes an unsupervised predictive memory method for goal-directed agents in reinforcement learning that improves sample efficiency.
- [ Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822) (2018): Proposes a relational recurrent neural network architecture for reinforcement learning that improves sample efficiency.
- [Imagination-Augmented Agents for Deep Reinforcement Learning](https://arxiv.org/abs/1707.06203) (2017): Proposes an imagination-augmented agent method for reinforcement learning that improves sample efficiency.
- [Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/abs/1708.02596) (2017): Proposes a neural network dynamics method for model-based deep reinforcement learning that improves sample efficiency.
- [Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning](https://arxiv.org/abs/1803.00101) (2018): Proposes a model-based value expansion method for model-free reinforcement learning that improves sample efficiency.
- [Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion](https://arxiv.org/abs/1807.01675) (2018): Proposes a sample-efficient reinforcement learning method with stochastic ensemble value expansion that improves sample efficiency.
- [Model-Ensemble Trust-Region Policy Optimization](https://openreview.net/forum?id=SJJinbWRZ&noteId=SJJinbWRZ) (2018): Proposes a model-ensemble trust-region policy optimization method for reinforcement learning that improves sample efficiency.
- [Model-Based Reinforcement Learning via Meta-Policy Optimization](https://arxiv.org/abs/1809.05214) (2018): Proposes a model-based reinforcement learning method via meta-policy optimization that improves sample efficiency.
- [Recurrent World Models Facilitate Policy Evolution](https://arxiv.org/abs/1809.01999) (2018): Proposes a recurrent world models method for policy evolution in reinforcement learning that improves sample efficiency.
- [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) (2017): Proposes a general reinforcement learning algorithm for mastering chess and shogi by self-play that achieves superhuman performance.
- [Thinking Fast and Slow with Deep Learning and Tree Search](https://arxiv.org/abs/1705.08439) (2017): Proposes a thinking fast and slow method for reinforcement learning that combines deep learning and tree search to improve sample efficiency.
- [RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning](https://arxiv.org/abs/1611.02779) (2016): Proposes an RL^2 method for fast reinforcement learning via slow reinforcement learning that improves sample efficiency.
- [Learning to Reinforcement Learn](https://arxiv.org/abs/1611.05763) (2016): Proposes a learning to reinforcement learn method for meta-reinforcement learning that improves sample efficiency.
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400) (2017): Proposes a model-agnostic meta-learning method for fast adaptation of deep networks in reinforcement learning that improves sample efficiency.
- [A Simple Neural Attentive Meta-Learner](https://openreview.net/forum?id=B1DmUzWAW&noteId=B1DmUzWAW) (2018): Proposes a simple neural attentive meta-learner method for meta-reinforcement learning that improves sample efficiency.
- [Accelerated Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1803.02811) (2018): Proposes accelerated methods for deep reinforcement learning that improve sample efficiency and learning speed.
- [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/abs/1802.01561) (2018): Proposes an importance weighted actor-learner architecture for scalable distributed deep reinforcement learning that improves sample efficiency and learning speed.
- [Distributed Prioritized Experience Replay](https://openreview.net/forum?id=H1Dy---0Z) (2018): Proposes a distributed prioritized experience replay method for deep reinforcement learning that improves sample efficiency and learning speed.
- [Recurrent Experience Replay in Distributed Reinforcement Learning](https://openreview.net/forum?id=r1lyTjAqYX) (2018): Proposes a recurrent experience replay method for distributed reinforcement learning that improves sample efficiency and learning speed.
- [RLlib: Abstractions for Distributed Reinforcement Learning](https://arxiv.org/abs/1712.09381) (2017): Proposes RLlib, a library of abstractions for distributed reinforcement learning that improves sample efficiency and learning speed. [(docs)](https://ray.readthedocs.io/en/latest/rllib.html)
- [Benchmarking Reinforcement Learning Algorithms on Real-World Robots](https://arxiv.org/abs/1809.07731) (2018): Conducts a benchmarking study of reinforcement learning algorithms on real-world robots.
- [Learning Dexterous In-Hand Manipulation](https://arxiv.org/abs/1808.00177) (2018): Proposes a method for learning dexterous in-hand manipulation skills in reinforcement learning that improves sample efficiency.
- [QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation](https://arxiv.org/abs/1806.10293) (2018): Proposes a scalable deep reinforcement learning method for vision-based robotic manipulation that improves sample efficiency.
- [Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform](https://arxiv.org/abs/1811.00260) (2018): Introduces Horizon, Facebook's open-source applied reinforcement learning platform.
- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) (2016): Discusses concrete problems in AI safety, including reinforcement learning.
- [Constrained Policy Optimization](https://arxiv.org/abs/1705.10528) (2017): Proposes a constrained policy optimization method for reinforcement learning that improves safety and stability.
- [Safe Exploration in Continuous Action Spaces](https://arxiv.org/abs/1801.08757) (2018): Proposes a safe exploration method for reinforcement learning in continuous action spaces that improves safety and stability.
- [Trial without Error: Towards Safe Reinforcement Learning via Human Intervention](https://arxiv.org/abs/1707.05173) (2017): Proposes a trial without error method for safe reinforcement learning via human intervention that improves safety and stability.
- [Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning](https://arxiv.org/abs/1711.06782) (2017): Proposes a learning to reset method for safe and autonomous reinforcement learning that improves safety and stability.
- [Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy](http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf) (2010): Proposes a principle of maximum causal entropy for modeling purposeful adaptive behavior in reinforcement learning.
- [Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization](https://arxiv.org/abs/1603.00448) (2016): Proposes a guided cost learning method for deep inverse optimal control via policy optimization in reinforcement learning.
- [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476) (2016): Proposes a generative adversarial imitation learning method for reinforcement learning that improves sample efficiency.
- [DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills](https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf) (2018): Proposes a deep mimic method for example-guided deep reinforcement learning of physics-based character skills that improves sample efficiency.
- [Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow](https://arxiv.org/abs/1810.00821) (2018): Proposes a variational discriminator bottleneck method for improving imitation learning, inverse RL, and GANs by constraining information flow in reinforcement learning.
- [One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL](https://arxiv.org/abs/1810.05017) (2018): Proposes a one-shot high-fidelity imitation method for training large-scale deep nets with reinforcement learning that improves sample efficiency.
- [Benchmarking Deep Reinforcement Learning for Continuous Control](https://arxiv.org/abs/1604.06778) (2016): Conducts a benchmarking study of deep reinforcement learning algorithms for continuous control.
- [Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control](https://arxiv.org/abs/1708.04133) (2017): Conducts a reproducibility study of benchmarked deep reinforcement learning tasks for continuous control.
- [Deep Reinforcement Learning that Matters](https://arxiv.org/abs/1709.06560) (2017): Discusses the importance of deep reinforcement learning research that addresses real-world problems.
- [Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods](https://arxiv.org/abs/1810.02525) (2018):  Conducts an empirical analysis of gradient descent optimization in policy gradient methods for reinforcement learning.
- [Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?](https://arxiv.org/abs/1811.02553) (2018): Discusses the definition and properties of policy gradient algorithms in reinforcement learning.
- [Simple Random Search Provides a Competitive Approach to Reinforcement Learning](https://arxiv.org/abs/1803.07055) (2018): Proposes a simple random search method for reinforcement learning that achieves competitive performance.
- [Benchmarking Model-Based Reinforcement Learning](https://arxiv.org/abs/1907.02057) (2019): Proposes a benchmarking library for (MBRL) algorithms and environments to facilitate research and comparison of MBRL methods.
- [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) (2000): Proposes a policy gradient method for reinforcement learning with function approximation that improves sample efficiency.
- [An Analysis of Temporal-Difference Learning with Function Approximation](http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf) (1997): Conducts an analysis of temporal-difference learning with function approximation in reinforcement learning.
- [Reinforcement Learning of Motor Skills with Policy Gradients](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf) (2008): Proposes a policy gradient method for reinforcement learning of motor skills that improves sample efficiency.
- [Approximately Optimal Approximate Reinforcement Learning](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf) (2002): Proposes an approximately optimal approximate reinforcement learning method that improves sample efficiency.
- [A Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf) (2002): Proposes a natural policy gradient method for reinforcement learning that improves sample efficiency.
- [Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf) (2009): Provides an overview of reinforcement learning algorithms, including model-based and model-free methods, and their applications.



## Quantum Machine Learning

- [PennyLane: Automatic differentiation of hybrid quantum-classical computations](https://arxiv.org/abs/1811.04968v4) (2018) [(code)](https://github.com/PennyLaneAI/pennylane)

- [TensorFlow Quantum: A Software Framework for Quantum Machine Learning](https://arxiv.org/abs/2003.02989v2) (2020) [(code)](https://github.com/tensorflow/quantum)

- [A divide-and-conquer algorithm for quantum state preparation](https://arxiv.org/abs/2008.01511v2) (2020) [(code)](https://github.com/qclib/qclib)

- [Quantum Neuron: an elementary building block for machine learning on quantum computers](https://arxiv.org/abs/1711.11240v1) (2017)

- [q-means: A quantum algorithm for unsupervised machine learning](https://arxiv.org/abs/1812.03584v2) (2019)
 
### Key Papers

- [Quantum Machine Learning](https://arxiv.org/abs/1611.09347) (2014) - Early paper outlining basic goals and approaches for quantum ML.
- [An introduction to quantum machine learning](https://arxiv.org/abs/1409.3097) (2014)
- [Distributed secure quantum machine learning](https://www.sciencedirect.com/science/article/abs/pii/S2095927317303250) (2017)
- [Quantum algorithms for supervised and unsupervised machine learning]() (2013) - Proposes quantum algorithms for supervised classification and unsupervised clustering.
- [Quantum Neural Networks]() (2018) - Describes a quantum version of neural networks with quantum circuits.
- [Alchemical and structural distribution based representation for universal quantum machine learning](https://pubs.aip.org/aip/jcp/article/148/24/241717/961132/Alchemical-and-structural-distribution-based) (2018)
- [Quantum variational autoencoder]() (2017) - Implements a variational autoencoder using parametrized quantum circuits.
- [Quantum Boltzmann Machine]() (2018) - Proposes a generative quantum model based on Boltzmann machines.
- [Quantum generative adversarial networks]() (2018) - Formulates a quantum version of GANs using quantum circuits.
- [Quantum graph neural networks]() (2019) - Applies quantum circuits for graph representation learning.
- [Quantum algorithms for topological and geometric analysis of data]() (2016) - Presents quantum algorithms for data topology/geometry.
- [Quantum reinforcement learning]() (2008) - An early approach to quantum reinforcement learning using superposition of states.

## Collections

- [Key Papers in Deep RL](https://spinningup.openai.com/en/latest/spinningup/keypapers.html#) by Open AI Spinning Up
